{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVAL CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. INTRODUCTION\n",
    "\n",
    "In this exercise we will see step by step the process of building a **generative chatbot**, preparing the dataset we want to work with, create a model and train it to get a dialog system with the ability to answer to the users questions. \n",
    "\n",
    "As you know from the slides, a generative chatbot generates an answer from scratch. It receives a question (the user input) and uses it to generate an answer based on the training performed.\n",
    "\n",
    "## 1.1. Dataset\n",
    "We are going to work again with the Ubuntu Corpus (https://arxiv.org/pdf/1506.08909) to create a generative chatbot capable of answering technical support questions about the well known OS Ubuntu. Other options are included in the project, such us Cornell Movie Corpus (movie dialogs) and a Custom Mode that allows you to use any dialog estructured corpus. Feel free to visit the project's Github (https://github.com/Conchylicultor/DeepQA) to know more about these datasets and experiment with them. \n",
    "\n",
    "\n",
    "\n",
    "## 1.2. Model\n",
    "The architecture of the neural network is called Sequence to Sequence or Encoder-Decoder. It receives a sentence, encode it to get an embedded representation of it (same way as was done with the retrieval chatbot). The difference is that now this representation will be fed to a **decoder**, that will reverse the process: it transforms the embedded representation into a set of words (the answer to the question). The original model can be found here: https://arxiv.org/pdf/1506.05869.pdf\n",
    "\n",
    "There's a maximum length of the sentence, used to discard all of the sentences longer than it and pad the shorter ones. The maximum length shouldn't be too big, as LSTM Recurrent Neural Networks are not proficient in remembering the first steps of a long sequence of steps. In this project we are going to work with 10 maximum words per sentence.\n",
    "\n",
    "<img src='basic_seq2seq.png'>\n",
    "\n",
    "*From the Seq2Seq Tensorflow tutorial (https://www.tensorflow.org/images/basic_seq2seq.png)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Requirements\n",
    "\n",
    "First of all, we need to install the libraries required to complete this project. The most important are:\n",
    "\n",
    "* Python >= 3\n",
    "* Tensorflow >= 1.0\n",
    "\n",
    "Once installed, import them into the project and we are ready to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse  # Command line parsing\n",
    "import configparser  # Saving the models parameters\n",
    "import datetime  # Chronometer\n",
    "import os  # Files management\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "from chatbot.textdata import TextData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Â 3. Setting everything up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Tensorflow graph, we have to define a series of variables that will be needed once we create the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = None\n",
    "\n",
    "# Task specific object\n",
    "textData = None  # Dataset\n",
    "model = None  # Sequence to sequence model\n",
    "\n",
    "# Tensorflow utilities for convenience saving/logging\n",
    "writer = None\n",
    "saver = None \n",
    "modelDir = ''  # Where the model is saved\n",
    "globStep = 0  # Represent the number of iteration for the current model\n",
    "\n",
    "# TensorFlow main session (we keep track for the daemon)\n",
    "sess = None\n",
    "\n",
    "# Filename and directories constants\n",
    "MODEL_DIR_BASE = 'save/model'\n",
    "MODEL_NAME_BASE = 'model'\n",
    "MODEL_EXT = '.ckpt'\n",
    "CONFIG_FILENAME = 'params.ini'\n",
    "CONFIG_VERSION = '0.5'\n",
    "TEST_IN_NAME = 'data/test/samples.txt'\n",
    "TEST_OUT_SUFFIX = '_predictions.txt'\n",
    "SENTENCES_PREFIX = ['Q: ', 'A: ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of testing, but in this demo we are going to use the interactive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestMode:\n",
    "    \"\"\" Simple structure representing the different testing modes\n",
    "    \"\"\"\n",
    "    ALL = 'all'\n",
    "    INTERACTIVE = 'interactive'  # The user can write his own questions\n",
    "    DAEMON = 'daemon'  # The chatbot runs on background and can regularly be called to predict something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the parameters that are going to be used to define our data preprocessing, model, training and testing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'test': None, # Options: None, TestMode.ALL, TestMode.INTERACTIVE, TestMode.DAEMON\n",
    "    'rootDir': '/notebooks', # Path where the project is loaded\n",
    "    'createDataset': False, # Just preprocess the dataset (no training or testing)\n",
    "    'reset': False, \n",
    "    'verbose': False,\n",
    "    'debug': False,\n",
    "    'keepAll': False,\n",
    "    'modelTag': 'generative', # Identificator for the model we are going to train/test\n",
    "    'watsonMode': False,\n",
    "    'autoEncode': False,\n",
    "    'playDataset': False,\n",
    "    'device': None,\n",
    "    'seed': 21111993,\n",
    "    'corpus': TextData.corpusChoices()[3], # Cornell, Ubuntu, Custom...\n",
    "    'datasetTag': 'course', # Identificator for the dataset we are going to use (None if not wanted)\n",
    "    'ratioDataset': 1.0, # Ratio of the dataset we want to use\n",
    "    'maxLength': 12, # Remove all sentences of the dataset with more than maxLength words\n",
    "    'filterVocab': 50, # Remove all words in the dataset that appear less times than filterVocab\n",
    "    'skipLines': False,\n",
    "    'vocabularySize': 40000, # Number of words in the vocabulary\n",
    "    'hiddenSize': 256, # Size of the hidden state of the LSTM cell\n",
    "    'numLayers': 2, # Number of layers of LSTM\n",
    "    'softmaxSamples': 2048,\n",
    "    'initEmbeddings': False, \n",
    "    'embeddingSize': 64, # Dimensions of the word embeddings\n",
    "    'embeddingSource': \"GoogleNews-vectors-negative300.bin\", # Pretrained word embeddings\n",
    "    'numEpochs': 30, # Number of epochs\n",
    "    'saveEvery': 500, # Save every N steps\n",
    "    'batchSize': 128, # Number of sentences to encode in each batch\n",
    "    'learningRate': 0.002, \n",
    "    'dropout': 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are going to be used to help with the saving and reloading of parameters. This is going to come useful to reload the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadModelParams():\n",
    "    \"\"\" Load the some values associated with the current model, like the current globStep value\n",
    "    For now, this function does not need to be called before loading the model (no parameters restored). However,\n",
    "    the modelDir name will be initialized here so it is required to call this function before managePreviousModel(),\n",
    "    _getModelName() or _getSummaryName()\n",
    "    Warning: if you modify this function, make sure the changes mirror saveModelParams, also check if the parameters\n",
    "    should be reset in managePreviousModel\n",
    "    \"\"\"\n",
    "    global args\n",
    "    global globStep\n",
    "    global modelDir\n",
    "    global MODEL_DIR_BASE\n",
    "    global CONFIG_FILENAME\n",
    "\n",
    "    # Compute the current model path\n",
    "    modelDir = os.path.join(args['rootDir'], MODEL_DIR_BASE)\n",
    "\n",
    "    if args['modelTag']:\n",
    "        modelDir += '-' + args['modelTag']\n",
    "\n",
    "    # If there is a previous model, restore some parameters\n",
    "    configName = os.path.join(modelDir, CONFIG_FILENAME)\n",
    "    if not args['reset'] and not args['createDataset'] and os.path.exists(configName):\n",
    "        # Loading\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(configName)\n",
    "\n",
    "        # Check the version\n",
    "        currentVersion = config['General'].get('version')\n",
    "        if currentVersion != CONFIG_VERSION:\n",
    "            raise UserWarning('Present configuration version {0} does not match {1}. You can try manual changes on \\'{2}\\''.format(currentVersion, CONFIG_VERSION, configName))\n",
    "\n",
    "        # Restoring the the parameters\n",
    "\n",
    "        globStep = config['General'].getint('globStep')\n",
    "        args['watsonMode'] = config['General'].getboolean('watsonMode')\n",
    "        args['autoEncode'] = config['General'].getboolean('autoEncode')\n",
    "        args['corpus'] = config['General'].get('corpus')\n",
    "\n",
    "        #args['datasetTag'] = config['Dataset'].get('datasetTag')\n",
    "        args['maxLength'] = config['Dataset'].getint('maxLength')  # We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength)\n",
    "        args['filterVocab'] = config['Dataset'].getint('filterVocab')\n",
    "\n",
    "        args['hiddenSize'] = config['Network'].getint('hiddenSize')\n",
    "        args['numLayers'] = config['Network'].getint('numLayers')\n",
    "        args['softmaxSamples'] = config['Network'].getint('softmaxSamples')\n",
    "        args['initEmbeddings'] = config['Network'].getboolean('initEmbeddings')\n",
    "        args['embeddingSize'] = config['Network'].getint('embeddingSize')\n",
    "        args['embeddingSource'] = config['Network'].get('embeddingSource')\n",
    "\n",
    "\n",
    "        # No restoring for training params, batch size or other non model dependent parameters\n",
    "\n",
    "        # Show the restored params\n",
    "        print()\n",
    "        print('Warning: Restoring parameters:')\n",
    "        print('globStep: {}'.format(globStep))\n",
    "        print('watsonMode: {}'.format(args['watsonMode']))\n",
    "        print('autoEncode: {}'.format(args['autoEncode']))\n",
    "        print('corpus: {}'.format(args['corpus']))\n",
    "        print('datasetTag: {}'.format(args['datasetTag']))\n",
    "        print('maxLength: {}'.format(args['maxLength']))\n",
    "        print('filterVocab: {}'.format(args['filterVocab']))\n",
    "        print('hiddenSize: {}'.format(args['hiddenSize']))\n",
    "        print('numLayers: {}'.format(args['numLayers']))\n",
    "        print('softmaxSamples: {}'.format(args['softmaxSamples']))\n",
    "        print('initEmbeddings: {}'.format(args['initEmbeddings']))\n",
    "        print('embeddingSize: {}'.format(args['embeddingSize']))\n",
    "        print('embeddingSource: {}'.format(args['embeddingSource']))\n",
    "        print()\n",
    "\n",
    "    # For now, not arbitrary  independent maxLength between encoder and decoder\n",
    "    args['maxLengthEnco'] = args['maxLength']\n",
    "    args['maxLengthDeco'] = args['maxLength'] + 2\n",
    "    \n",
    "def saveModelParams():\n",
    "    \"\"\" Save the params of the model, like the current globStep value\n",
    "    Warning: if you modify this function, make sure the changes mirror loadModelParams\n",
    "    \"\"\"\n",
    "    global args\n",
    "    global globStep\n",
    "    global modelDir\n",
    "    global CONFIG_FILENAME\n",
    "    global CONFIG_VERSION\n",
    "    config = configparser.ConfigParser()\n",
    "    config['General'] = {}\n",
    "    config['General']['version']  = CONFIG_VERSION\n",
    "    config['General']['globStep']  = str(globStep)\n",
    "    config['General']['watsonMode'] = str(args['watsonMode'])\n",
    "    config['General']['autoEncode'] = str(args['autoEncode'])\n",
    "    config['General']['corpus'] = str(args['corpus'])\n",
    "\n",
    "    config['Dataset'] = {}\n",
    "    #config['Dataset']['datasetTag'] = str(args['datasetTag'])\n",
    "    config['Dataset']['maxLength'] = str(args['maxLength'])\n",
    "    config['Dataset']['filterVocab'] = str(args['filterVocab'])\n",
    "    config['Dataset']['skipLines'] = str(args['skipLines'])\n",
    "    config['Dataset']['vocabularySize'] = str(args['vocabularySize'])\n",
    "\n",
    "    config['Network'] = {}\n",
    "    config['Network']['hiddenSize'] = str(args['hiddenSize'])\n",
    "    config['Network']['numLayers'] = str(args['numLayers'])\n",
    "    config['Network']['softmaxSamples'] = str(args['softmaxSamples'])\n",
    "    config['Network']['initEmbeddings'] = str(args['initEmbeddings'])\n",
    "    config['Network']['embeddingSize'] = str(args['embeddingSize'])\n",
    "    config['Network']['embeddingSource'] = str(args['embeddingSource'])\n",
    "\n",
    "    # Keep track of the learning params (but without restoring them)\n",
    "    config['Training (won\\'t be restored)'] = {}\n",
    "    config['Training (won\\'t be restored)']['learningRate'] = str(args['learningRate'])\n",
    "    config['Training (won\\'t be restored)']['batchSize'] = str(args['batchSize'])\n",
    "    config['Training (won\\'t be restored)']['dropout'] = str(args['dropout'])\n",
    "\n",
    "    with open(os.path.join(modelDir, CONFIG_FILENAME), 'w') as configFile:\n",
    "        config.write(configFile)\n",
    "\n",
    "def managePreviousModel(sess):\n",
    "    \"\"\" Restore or reset the model, depending of the parameters\n",
    "    If the destination directory already contains some file, it will handle the conflict as following:\n",
    "     * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\n",
    "     restart from scratch (globStep & cie reinitialized)\n",
    "     * Otherwise, it will depend of the directory content. If the directory contains:\n",
    "       * No model files (only summary logs): works as a reset (restart from scratch)\n",
    "       * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\n",
    "       decide by himself what to do\n",
    "       * The right model file (eventually some other): no problem, simply resume the training\n",
    "    In any case, the directory will exist as it has been created by the summary writer\n",
    "    Args:\n",
    "        sess: The current running session\n",
    "    \"\"\"\n",
    "    global args\n",
    "    global saver\n",
    "    global modelDir\n",
    "    print('WARNING: ', end='')\n",
    "\n",
    "    modelName = _getModelName()\n",
    "\n",
    "    if os.listdir(modelDir):\n",
    "        if args['reset']:\n",
    "            print('Reset: Destroying previous model at {}'.format(modelDir))\n",
    "        # Analysing directory content\n",
    "        elif os.path.exists(modelName):  # Restore the model\n",
    "            print('Restoring previous model from {}'.format(modelName))\n",
    "            saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\n",
    "        elif _getModelList():\n",
    "            print('Conflict with previous models.')\n",
    "            raise RuntimeError('Some models are already present in \\'{}\\'. You should check them first (or re-try with the keepAll flag)'.format(modelDir))\n",
    "        else:  # No other model to conflict with (probably summary files)\n",
    "            print('No previous model found, but some files found at {}. Cleaning...'.format(modelDir))  # Warning: No confirmation asked\n",
    "            args['reset'] = True\n",
    "\n",
    "        if args['reset']:\n",
    "            fileList = [os.path.join(modelDir, f) for f in os.listdir(modelDir)]\n",
    "            for f in fileList:\n",
    "                print('Removing {}'.format(f))\n",
    "                os.remove(f)\n",
    "\n",
    "    else:\n",
    "        print('No previous model found, starting from clean directory: {}'.format(modelDir))\n",
    "        \n",
    "def managePreviousModel( sess):\n",
    "    \"\"\" Restore or reset the model, depending of the parameters\n",
    "    If the destination directory already contains some file, it will handle the conflict as following:\n",
    "     * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\n",
    "     restart from scratch (globStep & cie reinitialized)\n",
    "     * Otherwise, it will depend of the directory content. If the directory contains:\n",
    "       * No model files (only summary logs): works as a reset (restart from scratch)\n",
    "       * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\n",
    "       decide by himself what to do\n",
    "       * The right model file (eventually some other): no problem, simply resume the training\n",
    "    In any case, the directory will exist as it has been created by the summary writer\n",
    "    Args:\n",
    "        sess: The current running session\n",
    "    \"\"\"\n",
    "    global args\n",
    "    global saver\n",
    "    global modelDir\n",
    "    print('WARNING: ', end='')\n",
    "\n",
    "    modelName = _getModelName()\n",
    "\n",
    "    if os.listdir(modelDir):\n",
    "        if args['reset']:\n",
    "            print('Reset: Destroying previous model at {}'.format(modelDir))\n",
    "        # Analysing directory content\n",
    "        elif os.path.exists(modelName):  # Restore the model\n",
    "            print('Restoring previous model from {}'.format(modelName))\n",
    "            saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\n",
    "        elif _getModelList():\n",
    "            print('Conflict with previous models.')\n",
    "            raise RuntimeError('Some models are already present in \\'{}\\'. You should check them first (or re-try with the keepAll flag)'.format(modelDir))\n",
    "        else:  # No other model to conflict with (probably summary files)\n",
    "            print('No previous model found, but some files found at {}. Cleaning...'.format(modelDir))  # Warning: No confirmation asked\n",
    "            args['reset'] = True\n",
    "\n",
    "        if args['reset']:\n",
    "            fileList = [os.path.join(modelDir, f) for f in os.listdir(modelDir)]\n",
    "            for f in fileList:\n",
    "                print('Removing {}'.format(f))\n",
    "                os.remove(f)\n",
    "\n",
    "    else:\n",
    "        print('No previous model found, starting from clean directory: {}'.format(modelDir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, a few more helping functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _getModelName():\n",
    "    \"\"\" Parse the argument to decide were to save/load the model\n",
    "    This function is called at each checkpoint and the first time the model is load. If keepAll option is set, the\n",
    "    globStep value will be included in the name.\n",
    "    Return:\n",
    "        str: The path and name were the model need to be saved\n",
    "    \"\"\"\n",
    "    global modelDir\n",
    "    global MODEL_NAME_BASE\n",
    "    global globStep\n",
    "    global args\n",
    "    global MODEL_EXT\n",
    "    \n",
    "    modelName = os.path.join(modelDir, MODEL_NAME_BASE)\n",
    "    if args['keepAll']:  # We do not erase the previously saved model by including the current step on the name\n",
    "        modelName += '-' + str(globStep)\n",
    "    return modelName + MODEL_EXT\n",
    "def _getModelList():\n",
    "    \"\"\" Return the list of the model files inside the model directory\n",
    "    \"\"\"\n",
    "    return [os.path.join(modelDir, f) for f in os.listdir(modelDir) if f.endswith(MODEL_EXT)]\n",
    "def _getSummaryName():\n",
    "    \"\"\" Parse the argument to decide were to save the summary, at the same place that the model\n",
    "    The folder could already contain logs if we restore the training, those will be merged\n",
    "    Return:\n",
    "        str: The path and name of the summary\n",
    "    \"\"\"\n",
    "    global modelDir\n",
    "    print(\"Model Dir:\", modelDir)\n",
    "    return modelDir\n",
    "def getDevice():\n",
    "    \"\"\" Parse the argument to decide on which device run the model\n",
    "    Return:\n",
    "        str: The name of the device on which run the program\n",
    "    \"\"\"\n",
    "    global args\n",
    "    if args['device'] == 'cpu':\n",
    "        return '/cpu:0'\n",
    "    elif args['device'] == 'gpu':\n",
    "        return '/gpu:0'\n",
    "    elif args['device'] is None:  # No specified device (default)\n",
    "        return None\n",
    "    else:\n",
    "        print('Warning: Error in the device name: {}, use the default device'.format(args['device']))\n",
    "        return None\n",
    "def _saveSession(sess):\n",
    "    \"\"\" Save the model parameters and the variables\n",
    "    Args:\n",
    "        sess: the current session\n",
    "    \"\"\"\n",
    "    tqdm.write('Checkpoint reached: saving model (don\\'t stop the run)...')\n",
    "    saveModelParams()\n",
    "    model_name = _getModelName()\n",
    "    with open(model_name, 'w') as f:  # HACK: Simulate the old model existance to avoid rewriting the file parser\n",
    "        f.write('This file is used internally by DeepQA to check the model existance. Please do not remove.\\n')\n",
    "    saver.save(sess, model_name)  # TODO: Put a limit size (ex: 3GB for the modelDir)\n",
    "    tqdm.write('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DATA PREPROCESSING\n",
    "\n",
    "All the data processing methods are included in the file **textdata.py** and **corpus/ubuntudata.py**. Unfortunately, there is too much code to be included in this notebook. We encourage you to go to the files and have a look to the code (it's commented). Overall, the most important points are the following ones:\n",
    "- **Tokens used**:\n",
    "    - *padToken*: token of the dictionary that will be used to pad the sentences to the max length allowed\n",
    "    - *goToken*: token of the dictionary that will be used to stop encoding and start the decoding of the input representation\n",
    "    - *eosToken*: token of the dictionary that will be used to indicate the end of the decoded sentence\n",
    "    - *unknownToken*: token of the dictionary that will be used to replace those words of the input sentence that are not in the dictionary\n",
    "\n",
    "\n",
    "- **Methods**:\n",
    "    - *_createBatch()*: create a batch from a group of samples\n",
    "    - *loadCorpus()*: first preprocess of the dataset, storing each dialog as a list of utterances\n",
    "    - *filterFromFull()*: filter the preprocessed dataset to match the args defined (maxLength, filterVocab...)\n",
    "    - *sentence2enco(sentence)*: translate the words in a sentence to the dictionary IDs \n",
    "    - *deco2sentence(decoderOutputs)*: translate a list of words IDs to a word sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DEFINING THE MODEL\n",
    "\n",
    "We are ready to start with the actual neural network! First we we are going to define a class skeleton with the methods that are going to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProjectionOp:\n",
    "    \"\"\" Single layer perceptron\n",
    "    Project input tensor on the output dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, scope=None, dtype=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shape: a tuple (input dim, output dim)\n",
    "            scope (str): encapsulate variables\n",
    "            dtype: the weights type\n",
    "        \"\"\"\n",
    "        assert len(shape) == 2\n",
    "\n",
    "        self.scope = scope\n",
    "\n",
    "        # Projection on the keyboard\n",
    "        with tf.variable_scope('weights_' + self.scope):\n",
    "            self.W_t = tf.get_variable(\n",
    "                'weights',\n",
    "                shape,\n",
    "                # initializer=tf.truncated_normal_initializer()  # TODO: Tune value (fct of input size: 1/sqrt(input_dim))\n",
    "                dtype=dtype\n",
    "            )\n",
    "            self.b = tf.get_variable(\n",
    "                'bias',\n",
    "                shape[0],\n",
    "                initializer=tf.constant_initializer(),\n",
    "                dtype=dtype\n",
    "            )\n",
    "            self.W = tf.transpose(self.W_t)\n",
    "\n",
    "    def getWeights(self):\n",
    "        \"\"\" Convenience method for some tf arguments\n",
    "        \"\"\"\n",
    "        return self.W, self.b\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\" Project the output of the decoder into the vocabulary space\n",
    "        Args:\n",
    "            X (tf.Tensor): input value\n",
    "        \"\"\"\n",
    "        with tf.name_scope(self.scope):\n",
    "            return tf.matmul(X, self.W) + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Implementation of a seq2seq model.\n",
    "    Architecture:\n",
    "        Encoder/decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, textData):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: parameters of the model\n",
    "            textData: the dataset object\n",
    "        \"\"\"\n",
    "        print(\"Model creation...\")\n",
    "\n",
    "        self.textData = textData  # Keep a reference on the dataset\n",
    "        self.args = args  # Keep track of the parameters of the model\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Placeholders\n",
    "        self.encoderInputs  = None\n",
    "        self.decoderInputs  = None  # Same that decoderTarget plus the <go>\n",
    "        self.decoderTargets = None\n",
    "        self.decoderWeights = None  # Adjust the learning to the target sentence size\n",
    "\n",
    "        # Main operators\n",
    "        self.lossFct = None\n",
    "        self.optOp = None\n",
    "        self.outputs = None  # Outputs of the network, list of probability for each words\n",
    "\n",
    "        # Construct the graphs\n",
    "        self.buildNetwork()\n",
    "    # Function to define the neural network\n",
    "    def buildNetwork(self):\n",
    "        return \n",
    "    # Function to define the operation and batches used for training/testing\n",
    "    def step(self, batch):\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to define the method that builds the neural network. It deserves special attention the function *create_rnn_cell()*. In this model, the selected **cell** is **LSTM**. The network is defined by the arguments *hiddenSize* and *numLayers*. The former defines the number of hidden states the LSTM layer has, while the later defines the number of LSTM layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildNetwork(self):\n",
    "\n",
    "    \"\"\" Create the computational graph\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Create name_scopes (for better graph visualisation)\n",
    "    # TODO: Use buckets (better perfs)\n",
    "\n",
    "    # Parameters of sampled softmax (needed for attention mechanism and a large vocabulary size)\n",
    "    outputProjection = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    if 0 < self.args['softmaxSamples'] < self.textData.getVocabularySize():\n",
    "        outputProjection = ProjectionOp(\n",
    "            (self.textData.getVocabularySize(), self.args['hiddenSize']),\n",
    "            scope='softmax_projection',\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        def sampledSoftmax(labels, inputs):\n",
    "            labels = tf.reshape(labels, [-1, 1])  # Add one dimension (nb of true classes, here 1)\n",
    "\n",
    "            # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "            # avoid numerical instabilities.\n",
    "            localWt     = tf.cast(outputProjection.W_t,             tf.float32)\n",
    "            localB      = tf.cast(outputProjection.b,               tf.float32)\n",
    "            localInputs = tf.cast(inputs,                           tf.float32)\n",
    "\n",
    "            return tf.cast(\n",
    "                tf.nn.sampled_softmax_loss(\n",
    "                    localWt,  # Should have shape [num_classes, dim]\n",
    "                    localB,\n",
    "                    labels,\n",
    "                    localInputs,\n",
    "                    self.args['softmaxSamples'],  # The number of classes to randomly sample per batch\n",
    "                    self.textData.getVocabularySize()),  # The number of classes\n",
    "                self.dtype)\n",
    "\n",
    "    # Creation of the rnn cell\n",
    "    def create_rnn_cell():\n",
    "        encoDecoCell = tf.contrib.rnn.BasicLSTMCell(  # Or GRUCell, LSTMCell(args['hiddenSize)\n",
    "            self.args['hiddenSize'],\n",
    "        )\n",
    "        if not self.args['test']:  # TODO: Should use a placeholder instead\n",
    "            encoDecoCell = tf.contrib.rnn.DropoutWrapper(\n",
    "                encoDecoCell,\n",
    "                input_keep_prob=1.0,\n",
    "                output_keep_prob=self.args['dropout']\n",
    "            )\n",
    "        return encoDecoCell\n",
    "    encoDecoCell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [create_rnn_cell() for _ in range(self.args['numLayers'])],\n",
    "    )\n",
    "\n",
    "    # Network input (placeholders)\n",
    "\n",
    "    with tf.name_scope('placeholder_encoder'):\n",
    "        self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args['maxLengthEnco'])]  # Batch size * sequence length * input dim\n",
    "\n",
    "    with tf.name_scope('placeholder_decoder'):\n",
    "        self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args['maxLengthDeco'])]  # Same sentence length for input and output (Right ?)\n",
    "        self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args['maxLengthDeco'])]\n",
    "        self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args['maxLengthDeco'])]\n",
    "\n",
    "    # Define the network\n",
    "    # Here we use an embedding model, it takes integer as input and convert them into word vector for\n",
    "    # better word representation\n",
    "    decoderOutputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "        self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args['maxLength\n",
    "        self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n",
    "        encoDecoCell,\n",
    "        self.textData.getVocabularySize(),\n",
    "        self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n",
    "        embedding_size=self.args['embeddingSize'],  # Dimension of each word\n",
    "        output_projection=outputProjection.getWeights() if outputProjection else None,\n",
    "        feed_previous=bool(self.args['test'])  # When we test (self.args['test']), we use previous output as next input (feed_previous)\n",
    "    )\n",
    "\n",
    "    # TODO: When the LSTM hidden size is too big, we should project the LSTM output into a smaller space (4086 => 2046): Should speed up\n",
    "    # training and reduce memory usage. Other solution, use sampling softmax\n",
    "\n",
    "    # For testing only\n",
    "    if self.args['test']:\n",
    "        if not outputProjection:\n",
    "            self.outputs = decoderOutputs\n",
    "        else:\n",
    "            self.outputs = [outputProjection(output) for output in decoderOutputs]\n",
    "\n",
    "        # TODO: Attach a summary to visualize the output\n",
    "\n",
    "    # For training only\n",
    "    else:\n",
    "        # Finally, we define the loss function\n",
    "        self.lossFct = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "            decoderOutputs,\n",
    "            self.decoderTargets,\n",
    "            self.decoderWeights,\n",
    "            self.textData.getVocabularySize(),\n",
    "            softmax_loss_function= sampledSoftmax if outputProjection else None  # If None, use default SoftMax\n",
    "        )\n",
    "        tf.summary.scalar('loss', self.lossFct)  # Keep track of the cost\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.args['learningRate'],\n",
    "            beta1=0.9,\n",
    "            beta2=0.999,\n",
    "            epsilon=1e-08\n",
    "        )\n",
    "        self.optOp = opt.minimize(self.lossFct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the model, we need to act differently in training than testing. \n",
    "\n",
    "- For **training**, we should feed the neural network with the inputs of both the encoder and the decoder, as we want to \"force\" the decoder with the actual answer. We are interested on retrieving the losses and the optimizer (Adam by default) operation to minimize the loss.\n",
    "\n",
    "- In **testing**, we only need to feed with the inputs of the encoder (the user question) and the operation to retrieve is the output of the decoder (answer of the chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(self, batch):\n",
    "\n",
    "    \"\"\" Forward/training step operation.\n",
    "    Does not perform run on itself but just return the operators to do so. Those have then to be run\n",
    "    Args:\n",
    "        batch (Batch): Input data on testing mode, input and target on output mode\n",
    "    Return:\n",
    "        (ops), dict: A tuple of the (training, loss) operators or (outputs,) in testing mode with the associated feed dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Feed the dictionary\n",
    "    feedDict = {}\n",
    "    ops = None\n",
    "\n",
    "    if not self.args['test']:  # Training\n",
    "        for i in range(self.args['maxLengthEnco']):\n",
    "            feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "        for i in range(self.args['maxLengthDeco']):\n",
    "            feedDict[self.decoderInputs[i]]  = batch.decoderSeqs[i]\n",
    "            feedDict[self.decoderTargets[i]] = batch.targetSeqs[i]\n",
    "            feedDict[self.decoderWeights[i]] = batch.weights[i]\n",
    "\n",
    "        ops = (self.optOp, self.lossFct)\n",
    "    else:  # Testing (batchSize == 1)\n",
    "        for i in range(self.args['maxLengthEnco']):\n",
    "            feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "        feedDict[self.decoderInputs[0]]  = [self.textData.goToken]\n",
    "        ops = (self.outputs,)\n",
    "\n",
    "    # Return one pass operator\n",
    "    return ops, feedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we update the class methods with them, and our model is ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model.buildNetwork = buildNetwork\n",
    "Model.step = step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our training function. We want to make sure if we have to restore an already existing model if we interrupted the training. After that, we get the batches for the training and, for each one of them, perform the operation with the feeding dictionary (as was defined in the previous part of the tutorial).\n",
    "\n",
    "A checkpoint of the model is saved every N steps, being N defined in the initial args (by default 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainTrain(sess):\n",
    "    \"\"\" Training loop\n",
    "    Args:\n",
    "        sess: The current running session\n",
    "    \"\"\"\n",
    "    global textData\n",
    "    global args\n",
    "    global writer\n",
    "    global model\n",
    "    global globStep\n",
    "    # Specific training dependent loading\n",
    "\n",
    "    textData.makeLighter(args['ratioDataset'])  # Limit the number of training samples\n",
    "\n",
    "    mergedSummaries = tf.summary.merge_all()  # Define the summary operator (Warning: Won't appear on the tensorboard graph)\n",
    "    if globStep == 0:  # Not restoring from previous run\n",
    "        writer.add_graph(sess.graph)  # First time only\n",
    "\n",
    "    # If restoring a model, restore the progression bar ? and current batch ?\n",
    "\n",
    "    print('Start training (press Ctrl+C to save and exit)...')\n",
    "\n",
    "    try:  # If the user exit while training, we still try to save the model\n",
    "        for e in range(args['numEpochs']):\n",
    "\n",
    "            print()\n",
    "            print(\"----- Epoch {}/{} ; (lr={}) -----\".format(e+1, args['numEpochs'], args['learningRate']))\n",
    "\n",
    "            batches = textData.getBatches()\n",
    "\n",
    "            # TODO: Also update learning parameters eventually\n",
    "\n",
    "            tic = datetime.datetime.now()\n",
    "            for nextBatch in tqdm(batches, desc=\"Training\"):\n",
    "                # Training pass\n",
    "                ops, feedDict = model.step(nextBatch)\n",
    "                assert len(ops) == 2  # training, loss\n",
    "                _, loss, summary = sess.run(ops + (mergedSummaries,), feedDict)\n",
    "                writer.add_summary(summary, globStep)\n",
    "                globStep += 1\n",
    "\n",
    "                # Output training status\n",
    "                if globStep % 100 == 0:\n",
    "                    perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "                    tqdm.write(\"----- Step %d -- Loss %.2f -- Perplexity %.2f\" % (globStep, loss, perplexity))\n",
    "\n",
    "                # Checkpoint\n",
    "                if globStep % args['saveEvery'] == 0:\n",
    "                    _saveSession(sess)\n",
    "\n",
    "            toc = datetime.datetime.now()\n",
    "\n",
    "            print(\"Epoch finished in {}\".format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn't really nicer\n",
    "    except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\n",
    "        print('Interruption detected, exiting the program...')\n",
    "\n",
    "    _saveSession(sess)  # Ultimate saving before complete exit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Welcome to DeepQA v0.1 !')\n",
    "print('TensorFlow detected: v{}'.format(tf.__version__))\n",
    "\n",
    "# General initialisation\n",
    "if not args['rootDir']:\n",
    "    args['rootDir'] = os.getcwd()  # Use the current working directory\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\n",
    "\n",
    "loadModelParams()  # Update the modelDir and globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\n",
    "\n",
    "# Data preprocessing function for the given arguments\n",
    "textData = TextData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model\n",
    "with tf.device(getDevice()):\n",
    "    model = Model(args, textData)\n",
    "\n",
    "# Saver/summaries\n",
    "writer = tf.summary.FileWriter(_getSummaryName())\n",
    "saver = tf.train.Saver(max_to_keep=200)\n",
    "\n",
    "# Running session\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # Allows backup device for non GPU-available operations (when forcing GPU)\n",
    "    log_device_placement=False)  # Too verbose ?\n",
    ")  # TODO: Replace all sess by sess (not necessary a good idea) ?\n",
    "\n",
    "if args['debug']:\n",
    "    sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "\n",
    "print('Initialize variables...')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\n",
    "if args['test'] != TestMode.ALL:\n",
    "    managePreviousModel(sess)\n",
    "\n",
    "# Initialize embeddings with pre-trained word2vec vectors\n",
    "if args['initEmbeddings']:\n",
    "    loadEmbedding(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: if Kernel dies during testing of a checkpoint and you need to execute the notebook again, you don't have to train it again. Please just skip the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "mainTrain(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. TESTING\n",
    "\n",
    "At this point we should have a trained model for the corpus. Now we want to make empirical tests to understand how good its answeres are, what are the weak and strong points and, finally, how to improve it.\n",
    "\n",
    "The same way we did with the training function, now we can define a testing function. Note that, although there are different types of teting in this project, in the tutorial we are going to focus on the **interactive** one. This mode allows the user to give a question as an input and receive an answer in an infinite loop (until you want the program to stop). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainTestInteractive( sess):\n",
    "    \"\"\" Try predicting the sentences that the user will enter in the console\n",
    "    Args:\n",
    "        sess: The current running session\n",
    "    \"\"\"\n",
    "    # TODO: If verbose mode, also show similar sentences from the training set with the same words (include in mainTest also)\n",
    "    # TODO: Also show the top 10 most likely predictions for each predicted output (when verbose mode)\n",
    "    # TODO: Log the questions asked for latter re-use (merge with test/samples.txt)\n",
    "    global SENTENCES_PREFIX\n",
    "    global textData\n",
    "    print('Testing: Launch interactive mode:')\n",
    "    print('')\n",
    "    print('Welcome to the interactive mode, here you can ask to Deep Q&A the sentence you want. Don\\'t have high '\n",
    "          'expectation. Type \\'exit\\' or just press ENTER to quit the program. Have fun.')\n",
    "\n",
    "    while True:\n",
    "        question = input(SENTENCES_PREFIX[0])\n",
    "        if question == '' or question == 'exit':\n",
    "            break\n",
    "\n",
    "        questionSeq = []  # Will be contain the question as seen by the encoder\n",
    "        answer = singlePredict(question, questionSeq)\n",
    "        if not answer:\n",
    "            print('Warning: sentence too long, sorry. Maybe try a simpler sentence.')\n",
    "            continue  # Back to the beginning, try again\n",
    "\n",
    "        print('{}{}'.format(SENTENCES_PREFIX[1], textData.sequence2str(answer, clean=True)))\n",
    "\n",
    "        if args['verbose']:\n",
    "            print(textData.batchSeq2str(questionSeq, clean=True, reverse=True))\n",
    "            print(textData.sequence2str(answer))\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction function, we listen for a question from the user. Once the input is submitted, we translate the words into IDs of our dictionary. These IDs are then transformed into embeddings, and these embeddings are fed to the network. The result after running the model is the output of the decoder. We need to translate the IDs into words and we have an answer to the user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singlePredict(question, questionSeq=None):\n",
    "    \"\"\" Predict the sentence\n",
    "    Args:\n",
    "        question (str): the raw input sentence\n",
    "        questionSeq (List<int>): output argument. If given will contain the input batch sequence\n",
    "    Return:\n",
    "        list <int>: the word ids corresponding to the answer\n",
    "    \"\"\"\n",
    "    # Create the input batch\n",
    "    batch = textData.sentence2enco(question)\n",
    "    if not batch:\n",
    "        return None\n",
    "    if questionSeq is not None:  # If the caller want to have the real input\n",
    "        questionSeq.extend(batch.encoderSeqs)\n",
    "\n",
    "    # Run the model\n",
    "    ops, feedDict = model.step(batch)\n",
    "\n",
    "    output = sess.run(ops[0], feedDict)  # TODO: Summarize the output too (histogram, ...)\n",
    "    answer = textData.deco2sentence(output)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we initialize our model to be tested. This should only be run once, if it's already loaded you might receive an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General initialisation\n",
    "args['test'] = TestMode.INTERACTIVE\n",
    "args['batchSize'] = 1\n",
    "\n",
    "if not args['rootDir']:\n",
    "    args['rootDir'] = os.getcwd()  # Use the current working directory\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\n",
    "\n",
    "loadModelParams()  # Update the modelDir and globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\n",
    "\n",
    "''' # Uncomment this block if you haven't loaded the model previously (not trained/testing performed since you opened this notebook)\n",
    "# Prepare the model\n",
    "textData = TextData(args)\n",
    "\n",
    "with tf.device(getDevice()):\n",
    "    model = Model(args, textData)\n",
    "'''\n",
    "# Saver/summaries\n",
    "writer = tf.summary.FileWriter(_getSummaryName())\n",
    "saver = tf.train.Saver(max_to_keep=200)\n",
    "\n",
    "# Running session\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # Allows backup device for non GPU-available operations (when forcing GPU)\n",
    "    log_device_placement=False)  # Too verbose ?\n",
    ")  # TODO: Replace all sess by sess (not necessary a good idea) ?\n",
    "\n",
    "if args['debug']:\n",
    "    sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "\n",
    "print('Initialize variables...')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Model dir value:\", modelDir)\n",
    "# Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\n",
    "if args['test'] != TestMode.ALL:\n",
    "    managePreviousModel(sess)\n",
    "\n",
    "# Initialize embeddings with pre-trained word2vec vectors\n",
    "if args['initEmbeddings']:\n",
    "    loadEmbedding(sess)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We have our model running. Now we just need to call the testing function to activate the interactive mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mainTestInteractive(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
