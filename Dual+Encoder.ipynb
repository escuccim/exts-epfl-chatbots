{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVAL CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. INTRODUCTION\n",
    "\n",
    "In this exercise we will see step by step the process of building a retrieval chatbot, preparing the dataset we want to work with, create a model and train the model to get a dialog system with the ability to answer to the users questions. \n",
    "\n",
    "As you know from the slides, a retrieval chatbot doesn't generate an answer from scratch. It receives a question (the user input), use some heuristic to retrieve a set of candidates to be answer to that question and finally it selects the best one as final answer. Our goal in this exercise is to have a chatbot able to perform this task in a closed domain: Ubuntu customer support.\n",
    "\n",
    "## 1.1. Dataset\n",
    "In this case we are going to work with the Ubuntu Corpus (https://arxiv.org/pdf/1506.08909) to create a retrieval chatbot capable of answering technical support questions about the well known OS Ubuntu. The set can be downloaded in **https://drive.google.com/file/d/0B_bZck-ksdkpVEtVc1R6Y01HMWM/view**. It consists of dialogs extracted from the forums, so each conversation has two participants. **Please, create a folder called 'data' in the same path where this notebook is being executed and extract there the dataset**\n",
    "\n",
    "In the training dataset, the dialogs have been processed to obtain a series of pairs **context** - **utterance**. Each sentence of the dialog is going to appear as an utterance in one of the pairs, while the context of that especific pair is formed by the sentences previous to the utterance.\n",
    "\n",
    "The testing dataset is different, as we have each sentence of the dialog as **context** and then the following sentence of the dialog (from the other user) as **utterance**. In addition, each pair has also 9 **distractors**, false utterances selected randomly from the dataset. Given a context, the model will receive the correct utterance and the distractors as candidates to be answers, and the model should be able to give the correct one a better score than the others.\n",
    "\n",
    "\n",
    "## 1.2. Model\n",
    "The architecture of the neural network is called the Dual Encoder LSTM. It's described also in the paper mentioned before, and it's formed by two encoders. One of them encodes the question we want to answer and the other one the candidate to be answer. The output of the architecture is a score between 0 and 1. The closer the score is to 1, the better the answer is for that question.\n",
    "\n",
    "<img src=\"dualencoder.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Requirements\n",
    "\n",
    "First of all, we need to install the libraries required to complete this project. The most important are:\n",
    "\n",
    "* Python = 2.7\n",
    "* Tensorflow = 0.12.1 \n",
    "\n",
    "Once installed, import them into the project and we are ready to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "from collections import namedtuple\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting everything up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Tensorflow graph, we have to define a series of variables that will be needed once we create the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data read/write Parameters\n",
    "tf.flags.DEFINE_string(\"input_dir\", \"./data\", \"Directory containing input data files 'train.tfrecords' and 'validation.tfrecords'\")\n",
    "tf.flags.DEFINE_integer(\"loglevel\", 20, \"Tensorflow log level\")\n",
    "\n",
    "# Training Parameters\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", None, \"Number of training Epochs. Defaults to indefinite.\")\n",
    "tf.flags.DEFINE_integer(\"eval_every\", 4000, \"Evaluate after this many train steps\")\n",
    "tf.flags.DEFINE_integer(\"steps\", 20000, \"Number of steps\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size during training\")\n",
    "tf.flags.DEFINE_string(\"optimizer\", \"Adam\", \"Optimizer Name (Adam, Adagrad, etc)\")\n",
    "\n",
    "# Evaluation/Testing Parameters\n",
    "tf.flags.DEFINE_integer(\"eval_steps\", 100, \"Number of steps\")\n",
    "tf.flags.DEFINE_integer(\"eval_batch_size\", 8, \"Batch size during evaluation\")\n",
    "tf.flags.DEFINE_integer(\"test_batch_size\", 8, \"Batch size for testing\")\n",
    "\n",
    "# Vocabulary Parameters\n",
    "tf.flags.DEFINE_integer(\"vocab_size\", 91620, \"The size of the vocabulary. Only change this if you changed the preprocessing\")\n",
    "\n",
    "# Pre-trained embeddings\n",
    "tf.flags.DEFINE_string(\"glove_path\", None, \"Path to pre-trained Glove vectors\")\n",
    "tf.flags.DEFINE_string(\"vocab_path\", None, \"Path to vocabulary.txt file\")\n",
    "\n",
    "# Prediction parameters\n",
    "tf.flags.DEFINE_string(\"vocab_processor_file\", \"./data/vocab_processor_p2.bin\", \"Saved vocabulary processor file\")\n",
    "\n",
    "# Model Parameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of the embeddings\")\n",
    "tf.flags.DEFINE_integer(\"rnn_dim\", 256, \"Dimensionality of the RNN cell\")\n",
    "tf.flags.DEFINE_integer(\"max_context_len\", 160, \"Truncate contexts to this length\")\n",
    "tf.flags.DEFINE_integer(\"max_utterance_len\", 80, \"Truncate utterance to this length\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although in this Notebook we can store global variables, it's convenient to save the parameters in the <b>Tensorflow Flags</b>. After defining them, we can initialize an object that contains them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HParams = namedtuple(\n",
    "  \"HParams\",\n",
    "  [\n",
    "    \"batch_size\",\n",
    "    \"embedding_dim\",\n",
    "    \"eval_batch_size\",\n",
    "    \"learning_rate\",\n",
    "    \"max_context_len\",\n",
    "    \"max_utterance_len\",\n",
    "    \"optimizer\",\n",
    "    \"rnn_dim\",\n",
    "    \"vocab_size\",\n",
    "    \"glove_path\",\n",
    "    \"vocab_path\"\n",
    "  ])\n",
    "\n",
    "def create_hparams():\n",
    "    return HParams(\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        eval_batch_size=FLAGS.eval_batch_size,\n",
    "        vocab_size=FLAGS.vocab_size,\n",
    "        optimizer=FLAGS.optimizer,\n",
    "        learning_rate=FLAGS.learning_rate,\n",
    "        embedding_dim=FLAGS.embedding_dim,\n",
    "        max_context_len=FLAGS.max_context_len,\n",
    "        max_utterance_len=FLAGS.max_utterance_len,\n",
    "        glove_path=FLAGS.glove_path,\n",
    "        vocab_path=FLAGS.vocab_path,\n",
    "        rnn_dim=FLAGS.rnn_dim)\n",
    "\n",
    "TIMESTAMP = int(time.time())\n",
    "\n",
    "# The directory where your model is going to be stored. Change the timestamp for a more recognizable name if you want\n",
    "\n",
    "MODEL_DIR = os.path.abspath(os.path.join(\"./runs\", str(TIMESTAMP)))\n",
    "MODEL_DIR_PRETRAINED = os.path.abspath(os.path.join(\"./runs\", \"1486584016/\"))\n",
    "\n",
    "print(MODEL_DIR)\n",
    "\n",
    "TRAIN_FILE = os.path.abspath(os.path.join(FLAGS.input_dir, \"train.tfrecords\"))\n",
    "VALIDATION_FILE = os.path.abspath(os.path.join(FLAGS.input_dir, \"validation.tfrecords\"))\n",
    "TEST_FILE = os.path.abspath(os.path.join(FLAGS.input_dir, \"test.tfrecords\"))\n",
    "\n",
    "# Level of logging\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# We initialize the object that will be used in the model to access the variables.\n",
    "hparams = create_hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Description of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our parameters initialized, it's time to start defining the <b>Graph</b> of the model. In first place, we define a function to initialize the <b>embedding matrix</b>. Its shape is <i>Number of words in the vocabulary</i> x <i>Number of embedding dimensions</i>, both parameters defined in the global variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(hparams):\n",
    "    tf.logging.info(\"Starting with random embeddings.\")\n",
    "    initializer = tf.random_uniform_initializer(-0.25, 0.25)\n",
    "    return tf.get_variable(\n",
    "        \"word_embeddings\",\n",
    "        shape=[hparams.vocab_size, hparams.embedding_dim],\n",
    "        initializer=initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define our <b>model</b>. Using the last function, we can get the embedding matrix with the desired dimensions. From that matrix, we translate both the context and the utterance to get the embedded representation of the sentences. \n",
    "\n",
    "Once we have them, we define what type of **cell** we want to use for the Recurrent Neural Network. In this model we are going to use **LSTM**, although GRU could also work. Then, we use that cell to define the complete RNN which will be used as encoder. We concatenate both embedded sentences and pass it through the RNN, obtaining at the end the encoded context and utterance.\n",
    "\n",
    "Finally, the **prediction** for a given context will be obtained by multiplying the context encoded by the *prediction matrix* M, that will be trained. However, it isn't this prediction what we want to get. Now, we can multiply it to the real encoded utterance and apply the *sigmoid* function to get the probability of the pair context-utterance being correct. \n",
    "\n",
    "For the testing we will return this score as a result. However, for the training we must minimize the **mean loss** for each batch. The chosen loss function is the **cross entropy**, as in the training dataset we have labelled whether each utterance belongs to the context. \n",
    "\n",
    "Thanks to that, if the label is 1 (the pair is correct) the loss will be very close to 0 only if the score given is high, penalizing the mistake. The same works for the other case, being the label 0 (the pair is wrong), if the score is high then it will be penalized as the loss will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dual_encoder_model(\n",
    "    hparams,\n",
    "    mode,\n",
    "    context,\n",
    "    context_len,\n",
    "    utterance,\n",
    "    utterance_len,\n",
    "    targets):\n",
    "\n",
    "    # Initialize embedidngs randomly or with pre-trained vectors if available\n",
    "    embeddings_W = get_embeddings(hparams)\n",
    "\n",
    "    # Embed the context and the utterance\n",
    "    context_embedded = tf.nn.embedding_lookup(\n",
    "        embeddings_W, context, name=\"embed_context\")\n",
    "    utterance_embedded = tf.nn.embedding_lookup(\n",
    "        embeddings_W, utterance, name=\"embed_utterance\")\n",
    "\n",
    "    # Build the RNN\n",
    "    with tf.variable_scope(\"rnn\") as vs:\n",
    "    # We use an LSTM Cell\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(\n",
    "            hparams.rnn_dim,\n",
    "            forget_bias=2.0,\n",
    "            use_peepholes=True,\n",
    "            state_is_tuple=True)\n",
    "\n",
    "        # Run the utterance and context through the RNN\n",
    "        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            tf.concat(0, [context_embedded, utterance_embedded]),\n",
    "            sequence_length=tf.concat(0, [context_len, utterance_len]),\n",
    "            dtype=tf.float32)\n",
    "        encoding_context, encoding_utterance = tf.split(0, 2, rnn_states.h)\n",
    "\n",
    "    with tf.variable_scope(\"prediction\") as vs:\n",
    "        M = tf.get_variable(\"M\",\n",
    "          shape=[hparams.rnn_dim, hparams.rnn_dim],\n",
    "          initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "        # \"Predict\" a  response: c * M\n",
    "        generated_response = tf.matmul(encoding_context, M)\n",
    "        generated_response = tf.expand_dims(generated_response, 2)\n",
    "        encoding_utterance = tf.expand_dims(encoding_utterance, 2)\n",
    "\n",
    "        # Dot product between generated response and actual response\n",
    "        # (c * M) * r\n",
    "        logits = tf.batch_matmul(generated_response, encoding_utterance, True)\n",
    "        logits = tf.squeeze(logits, [2])\n",
    "\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probs = tf.sigmoid(logits)\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            return probs, None\n",
    "\n",
    "        # Calculate the binary cross-entropy loss\n",
    "        losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(targets))\n",
    "\n",
    "    # Mean loss across the batch of examples\n",
    "    mean_loss = tf.reduce_mean(losses, name=\"mean_loss\")\n",
    "    return probs, mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined our parameters and prepared the model for training and testing. Now, we need to actually **train** the model. During this process we are going to train different variables: vocabulary embeddings, prediction matrix and RNN parameters. We are going to create a function that returns the function to be used, depending on the mode (training/testing/evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some helper functions. With *get_id_feature()* we can retrieve the list of IDs of the words of a sentence and its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_feature(features, key, len_key, max_len):\n",
    "    ids = features[key]\n",
    "    ids_len = tf.squeeze(features[len_key], [1])\n",
    "    ids_len = tf.minimum(ids_len, tf.constant(max_len, dtype=tf.int64))\n",
    "    return ids, ids_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the function that is going to perform the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_op(loss, hparams):\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.contrib.framework.get_global_step(),\n",
    "        learning_rate=hparams.learning_rate,\n",
    "        clip_gradients=10.0,\n",
    "        optimizer=hparams.optimizer)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to create the whole training process, we create a function that returns the function we need for the training. The explanation for this is that we might want to create new models to experiment with and thanks to this we can just create or modify the model without changing the next function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_fn(hparams, model_impl):\n",
    "    def model_fn(features, targets, mode):\n",
    "        context, context_len = get_id_feature(\n",
    "            features, \"context\", \"context_len\", hparams.max_context_len)\n",
    "\n",
    "        utterance, utterance_len = get_id_feature(\n",
    "            features, \"utterance\", \"utterance_len\", hparams.max_utterance_len)\n",
    "        \n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            probs, loss = model_impl(\n",
    "                  hparams,\n",
    "                  mode,\n",
    "                  context,\n",
    "                  context_len,\n",
    "                  utterance,\n",
    "                  utterance_len,\n",
    "                  targets)\n",
    "            train_op = create_train_op(loss, hparams)\n",
    "            return probs, loss, train_op\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            probs, loss = model_impl(\n",
    "                hparams,\n",
    "                mode,\n",
    "                context,\n",
    "                context_len,\n",
    "                utterance,\n",
    "                utterance_len,\n",
    "                None)\n",
    "            return probs, 0.0, None\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "            batch_size = targets.get_shape().as_list()[0]\n",
    "            # We have 10 exampels per record, so we accumulate them\n",
    "            all_contexts = [context]\n",
    "            all_context_lens = [context_len]\n",
    "            all_utterances = [utterance]\n",
    "            all_utterance_lens = [utterance_len]\n",
    "            all_targets = [tf.ones([batch_size, 1], dtype=tf.int64)]\n",
    "\n",
    "            for i in range(9):\n",
    "                distractor, distractor_len = get_id_feature(features,\n",
    "                    \"distractor_{}\".format(i),\n",
    "                    \"distractor_{}_len\".format(i),\n",
    "                    hparams.max_utterance_len)\n",
    "                all_contexts.append(context)\n",
    "                all_context_lens.append(context_len)\n",
    "                all_utterances.append(distractor)\n",
    "                all_utterance_lens.append(distractor_len)\n",
    "                all_targets.append(\n",
    "                  tf.zeros([batch_size, 1], dtype=tf.int64)\n",
    "                )\n",
    "\n",
    "            probs, loss = model_impl(\n",
    "                hparams,\n",
    "                mode,\n",
    "                tf.concat(0, all_contexts),\n",
    "                tf.concat(0, all_context_lens),\n",
    "                tf.concat(0, all_utterances),\n",
    "                tf.concat(0, all_utterance_lens),\n",
    "                tf.concat(0, all_targets))\n",
    "\n",
    "            split_probs = tf.split(0, 10, probs)\n",
    "            shaped_probs = tf.concat(1, split_probs)\n",
    "\n",
    "            # Add summaries\n",
    "            tf.histogram_summary(\"eval_correct_probs_hist\", split_probs[0])\n",
    "            tf.scalar_summary(\"eval_correct_probs_average\", tf.reduce_mean(split_probs[0]))\n",
    "            tf.histogram_summary(\"eval_incorrect_probs_hist\", split_probs[1])\n",
    "            tf.scalar_summary(\"eval_incorrect_probs_average\", tf.reduce_mean(split_probs[1]))\n",
    "\n",
    "            return shaped_probs, loss, None\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model function, we want to define the function that is going to prepare the data to be used in batches for training/evaluating/testing. We are going to follow the same proccedure as before, defining a function that returns nother customiezed function depending on the mode and other parameteres passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper fucntion to store the information depending on the MODE used \n",
    "TEXT_FEATURE_SIZE = 160\n",
    "\n",
    "def get_feature_columns(mode):\n",
    "    feature_columns = []\n",
    "\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(column_name=\"context\", dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(column_name=\"context_len\", dimension=1, dtype=tf.int64))\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(column_name=\"utterance\", dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(column_name=\"utterance_len\", dimension=1, dtype=tf.int64))\n",
    "\n",
    "    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "        # During training we have a label feature to know if the pair context-utterance is correct\n",
    "        feature_columns.append(tf.contrib.layers.real_valued_column(column_name=\"label\", dimension=1, dtype=tf.int64))\n",
    "\n",
    "    if mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "        # During evaluation we have 9 distractors\n",
    "        for i in range(9):\n",
    "            feature_columns.append(tf.contrib.layers.real_valued_column(column_name=\"distractor_{}\".format(i), dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n",
    "            feature_columns.append(tf.contrib.layers.real_valued_column(column_name=\"distractor_{}_len\".format(i), dimension=1, dtype=tf.int64))\n",
    "\n",
    "    return set(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the data to be used in batches\n",
    "def create_input_fn(mode, input_files, batch_size, num_epochs):\n",
    "    def input_fn():\n",
    "        features = tf.contrib.layers.create_feature_spec_for_parsing(get_feature_columns(mode))\n",
    "\n",
    "        feature_map = tf.contrib.learn.io.read_batch_features(\n",
    "            file_pattern=input_files,\n",
    "            batch_size=batch_size,\n",
    "            features=features,\n",
    "            reader=tf.TFRecordReader,\n",
    "            randomize_input=True,\n",
    "            num_epochs=num_epochs,\n",
    "            queue_capacity=200000 + batch_size * 10,\n",
    "            name=\"read_batch_features_{}\".format(mode))\n",
    "\n",
    "        # This is an ugly hack because of a current bug in tf.learn\n",
    "        # During evaluation TF tries to restore the epoch variable which isn't defined during training\n",
    "        # So we define the variable manually here\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            tf.get_variable(\"read_batch_features_eval/file_name_queue/limit_epochs/epochs\",\n",
    "                initializer=tf.constant(0, dtype=tf.int64))\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            target = feature_map.pop(\"label\")\n",
    "        else:\n",
    "            # In evaluation we have 10 classes (utterances).\n",
    "            # The first one (index 0) is always the correct one and the other 9 are the distractors\n",
    "            target = tf.zeros([batch_size, 1], dtype=tf.int64)\n",
    "        return feature_map, target\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the evaluation metric we are going to define recall@k, as was described before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_evaluation_metrics():\n",
    "    eval_metrics = {}\n",
    "    for k in [1, 2, 5, 10]:\n",
    "        eval_metrics[\"recall_at_%d\" % k] = MetricSpec(metric_fn=functools.partial(\n",
    "            tf.contrib.metrics.streaming_sparse_recall_at_k,\n",
    "            k=k))\n",
    "    return eval_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, now we just have to specify the model we want to train with to obtain a valid function. After all the functions have been created, we can initialize our model, estimator, training input, metrics and monitor to see how the loss evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model, specifying that we want the Dual Encoder (the only one we have)\n",
    "model_fn = create_model_fn(\n",
    "    hparams,\n",
    "    model_impl=dual_encoder_model)\n",
    "\n",
    "# Initialize the estimator\n",
    "estimator = tf.contrib.learn.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=MODEL_DIR,\n",
    "    config=tf.contrib.learn.RunConfig())\n",
    "\n",
    "# Initialize the input batch estructure for the training\n",
    "input_fn_train = create_input_fn(\n",
    "    mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "    input_files=[TRAIN_FILE],\n",
    "    batch_size=hparams.batch_size,\n",
    "    num_epochs=FLAGS.num_epochs)\n",
    "\n",
    "# Same with the evaluation input, to monitor the accuracy on testing during the training\n",
    "input_fn_eval = create_input_fn(\n",
    "    mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "    input_files=[VALIDATION_FILE],\n",
    "    batch_size=hparams.eval_batch_size,\n",
    "    num_epochs=1)\n",
    "\n",
    "# Initialize the metric we are going to use to measure the accuracy of the model (recall@k)\n",
    "eval_metrics = create_evaluation_metrics()\n",
    "\n",
    "# We put the evaluation data and the metric together to monitorize the accuracy\n",
    "eval_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    input_fn=input_fn_eval,\n",
    "    every_n_steps=FLAGS.eval_every,\n",
    "    metrics=eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Everything is ready for the training!** Note that on the original experiment they trained for 20000 steps, which can take long (about 30-40 hours) without a GPU. Feel free to change the number of steps, although the results can be notably worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(input_fn=input_fn_train, steps=FLAGS.steps, monitors=[eval_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. EVALUATING\n",
    "\n",
    "Now that we have a model fit, we can test it with the evaluation set included in the Ubuntu Corpus. Remember the estructure:\n",
    "* Context\n",
    "* Correct utterance\n",
    "* Nine distractors (wrong utterances)\n",
    "\n",
    "The evaluation is based on the function **recall@k**, being k the size of the subset selected. In other words, for each context the model will evaluate all 10 possible utterances and assign a score to each of them. For recall@1 only is correct if the best score is the correct utterance, for recall@5 it's considered correct if the correct utterance is between the 5 best scores, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**: From now on, if you want to evaluate/test with the provided checkpoint, please use \"estimator_pretrained\" as a variable instead of \"estimator\". If you have trained your own estimator, then just keep using the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator_pretrained = tf.contrib.learn.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=MODEL_DIR_PRETRAINED,\n",
    "    config=tf.contrib.learn.RunConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_fn_test = create_input_fn(\n",
    "    mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "    input_files=[TEST_FILE],\n",
    "    batch_size=FLAGS.test_batch_size,\n",
    "    num_epochs=1)\n",
    "\n",
    "# use estimator_pretrained if using predefined checkpoint\n",
    "estimator.evaluate(input_fn=input_fn_test, steps=FLAGS.steps, metrics=eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MAKING PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to remember that the main goal of this course is to be able to build a chatbot that it's able to interact with human beings. That means that it should be able to **give answers to questions outside the dataset**. For that, everytime a question is asked we can retrieve a set of possible answers and pass them by the model to obtain the score. After all the process is gone, we select the one with best score as the answer that will be returned to the user!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer_fn(iterator):\n",
    "  return (x.split(\" \") for x in iterator)\n",
    "\n",
    "# Load vocabulary\n",
    "vp = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(FLAGS.vocab_processor_file)\n",
    "\n",
    "# Load your own data here\n",
    "INPUT_CONTEXT = \"How can I remove a file\"\n",
    "POTENTIAL_RESPONSES = [\"what do you mean?\", \"rm -r\", \"top\", \"ifconfig\"]\n",
    "\n",
    "def get_features(context, utterance):\n",
    "  context_matrix = np.array(list(vp.transform([context])))\n",
    "  utterance_matrix = np.array(list(vp.transform([utterance])))\n",
    "  context_len = len(context.split(\" \"))\n",
    "  utterance_len = len(utterance.split(\" \"))\n",
    "  features = {\n",
    "    \"context\": tf.convert_to_tensor(context_matrix, dtype=tf.int64),\n",
    "    \"context_len\": tf.constant(context_len, shape=[1,1], dtype=tf.int64),\n",
    "    \"utterance\": tf.convert_to_tensor(utterance_matrix, dtype=tf.int64),\n",
    "    \"utterance_len\": tf.constant(utterance_len, shape=[1,1], dtype=tf.int64),\n",
    "  }\n",
    "  return features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ugly hack, seems to be a bug in Tensorflow\n",
    "# estimator.predict doesn't work without this line\n",
    "# use estimator_pretrained if using predefined checkpoint\n",
    "estimator._targets_info = tf.contrib.learn.estimators.tensor_signature.TensorSignature(tf.constant(0, shape=[1,1]))\n",
    "\n",
    "\n",
    "# use estimator_pretrained if using predefined checkpoint\n",
    "print(\"Context: {}\".format(INPUT_CONTEXT))\n",
    "for r in POTENTIAL_RESPONSES:\n",
    "    prob = estimator.predict(input_fn=lambda: get_features(INPUT_CONTEXT, r))\n",
    "    results = next(prob)\n",
    "    print(r, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "However, in the last step we have cheated. We have manually added the candidates to be evaluated, but this is not going to be possible in a real world scenario. For that, we came up with the idea of using <b>Solr</b>. Solr gives you the opportunity (among many others that we don't need here) of indexing the whole dataset and performing similarity queries in it.\n",
    "\n",
    "The best way to perform the indexing is by creating an appropiate estructure of the data. We are going to need to query the user input (the question) against the database, select a group of the most similar existing questions and get the answer of the other user in the Ubuntu forum to be evaluated. Each sentence in the dataset can be stored with the following information:\n",
    "\n",
    "- **author**: name of the user that wrote the sentence\n",
    "- **recipient**: name of the other user present in the dialog\n",
    "- **content**: the sentence (can be considered the <i>answer</i>\n",
    "- **responseTo**: the last sentence from the other user that came before this one (can be considered the <i>question</i>)\n",
    "\n",
    "With this estructure in Solr we can query by the user question to the chatbot against the <i>responseTo</i> field of all the stored sentences. The ones with biggest Solr similarity score are the sentences that have the best probability to be asking the same questions as the user, so we can take their <i>content</i> field and add them to the set of possible answers to return to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "solr_server = \"http://localhost:8983/solr/\"\n",
    "col_name = \"ubuntu_corpus/\"\n",
    "def predict(raw_query, number_results):\n",
    "    query = raw_query.replace(\" \", \"%20\")\n",
    "\n",
    "    url_query = solr_server + col_name + 'select?defType=edismax&indent=on&bq=responseTo:[*%20TO%20*]^5&q.alt=' + query + '&qf=responseTo&rows=' + str(number_results) + '&wt=json'\n",
    "\n",
    "    r = requests.get(url_query).json()\n",
    "\n",
    "    candidates_objects = r['response']['docs']\n",
    "    candidates = [ c['content'] for c in candidates_objects ]\n",
    "    print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(\"what is the command to remove a file?\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
